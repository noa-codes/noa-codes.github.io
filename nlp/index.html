<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Natural Language Processing</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="theme-color" content="#D50000">
<meta name="description" content="">
<meta property="og:description" content="">
<meta name="twitter:description" content="">
<meta property="og:image" content="architecture_mini.png">
<meta name="twitter:image" content="architecture_mini.png">
<link rel="icon" href="https://noa-codes.github.io/img/favicon.png" type="image/x-icon">
<link rel="shortcut icon" href="https://noa-codes.github.io/img/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="https://noa-codes.github.io/css/final.css">
<link rel="stylesheet" href="https://noa-codes.github.io/css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Merriweather" rel="stylesheet">

    </head>
    <body>
        <nav class="main-navigation stuckMenu">
    <a class="logo" href="https://noa-codes.github.io/">Noa Bendit-Shtull</a>
    
    <div class="sub-navigation">
        <a href="https://noa-codes.github.io/" class="main-navigation-link" data-scroll-goto="2">Projects</a>
    </div>
</nav>
        <section>
    <div class="main-case-study">
        <h1 class="title" style="text-align: center;">Modeling Long-Distance Dependencies with Second-Order LSTMs</h1>
        <h2 class="subtitle" style="text-align: center;">Natural Language Processing</h2>
        <img src="img/architecture.png" alt="">
        <p class="summary">A second-order LSTM is a modified LSTM architecture that conditionally selects one of multiple weight matrices to apply to each input, based on that input. Existing research on second-order RNNs focuses on character-level language modeling. Our task is to develop a second-order LSTM for word-level language models. Specifically, we test the hypothesis that second-order LSTM architectures have the improved ability (relative to a simple LSTM) to learn long distance dependencies within an input. We find that, although second-order models don’t exhibit quantitative improvement, they can be trained in fewer epochs and are able to recover from unexpected inputs more quickly.</p>
        <div class="case-details">
            <div class="case-details-item">
                <p><strong>Team</strong>
                    
                    <br>Richard Lin
                    
                    <br>Samuel Spinner
                    
                    <br>Noa Bendit-Shtull
                    
                </p>
            </div>

        </div>
        <h1 style="color: black; font-family: merriweather; font-size: 26px; line-height: 1.7em; margin-top: 20px;"></h1>
        <div class="case-links">
            
            
            <div class="link-div">
                <a href="http://web.stanford.edu/class/cs224n/reports/custom/report44.pdf" target="blank" class="case-detail-link">
                    
                    <img style="top:1.8px;" src="https://noa-codes.github.io/svg/download.svg">
                    
                    Download Final Deliverable
                </a>
            </div>
            
        </div>
    </div>
</section>

        <div class="content-case-study">
            <h1 id="study-overview">Study Overview</h1>
<p>In the basic RNN, the vanishing gradient problem limits a language model&rsquo;s memory; a range of models, such as LSTM and GRU, have been developed to reduce this friction. In order to preserve long-term memory, these models must determine which information to store from previous hidden states. This introduces a related problem–if the model chooses the wrong information to carry in its long-term memory, it may be difficult to recover. As Kraus et al. explain,</p>
<blockquote>
<p>&ldquo;If the RNN’s hidden representation remembers the wrong information and reaches a bad numerical state for predicting future sequence elements, for instance as a result of an unexpected input, it may take many time-steps to recover&rdquo; (Kraus et al. 2017)</p>
</blockquote>
<p>This paper contributes to a small body of existing work focused on developing new architectures to improve RNN performance on modeling long-range dependencies; that is, to help RNNs remember the right information. Our goal is to assess whether a second-order LSTM – an LSTM that routes different inputs to different LSTM cells – can make progress in this research area.</p>
<h1 id="approach">Approach</h1>
<p>Our project is divided into two stages, a proof-of-concept and an application to natural language. The proof-of-concept uses synthetic language, specifically the family of m-bounded Dyck-k (in this case, Dyck-2) languages. The second stage of the project extends the analysis to natural language, where successful modeling of long-distance dependency is more difficult to quantify.</p>
<h2 id="baseline">Baseline</h2>
<p>This baseline model consists of a single layer LSTM and the hidden states at each time step are passed through a fully connected layer to generate the predicted probability distribution across all vocabulary words.</p>
<h2 id="second-order-model">Second-Order Model</h2>
<p>An $S$-dimensional second-order model contains $S$ LSTM cells, with $4S$ transition matrices. In the attention-based second-order LSTM model, the attention mechanism sends each input to all LSTM cells, and uses learned attention scores to compute a linear combination of the hidden and cell state outputs. Specifically, an $S$-dimensional second-order LSTM has attention matrix V with dimension <code>$S \times embedding\_dim$</code>. For each of the LSTM cells, we compute the intermediate hidden state and memory cell state <code>$h_t$</code> and <code>$c_t$</code> (represented in the figure above as joint <code>$\widehat{h}_t$</code>), and then use attention to compute a linear combination of the intermediate states to get the new states <code>$h_t$</code> and <code>$c_t$</code>, as follows:</p>
<p><code>$\begin{align} &amp;e_t = V x_t \\ &amp;\alpha_t = \text{softmax}\left(\frac{e_t}{\tau}\right) \\ &amp;h_t^{(s)}, c_t^{(s)} = \text{LSTMCell}^{(s)}(x_t, (h_{t-1}, c_{t-1})) \\ &amp;h_{t} = \sum_{s=1}^{S} \alpha_{t, s} \cdot h_t^{(s)} \\ &amp;c_{t} = \sum_{s=1}^{S} \alpha_{t, s} \cdot c_t^{(s)} \end{align}$</code></p>
<p>where $\tau$ is a temperature parameter that affects the outcome of the softmax function. Temperature is initialized &ldquo;hot&rdquo; with $\tau = 1$ (making (b) effectively a normal softmax), but then decreases with each epoch by a constant multiplier. The smaller the value of $\tau$, the more probability mass is put on one LSTM cell. When $\tau \approx 0.1$, (b) is effectively one-hot. We decay the temperature throughout training so that eventually each input word gets assigned to one particular LSTM cell and the hidden state output by that cell is effectively the next hidden state.</p>
<h1 id="experiments">Experiments</h1>
<h2 id="data">Data</h2>
<p>The $m$-bounded Dyck-$k$ data are comprised of $k$ unique types of parentheses, with at most <code>$m$</code> unclosed parentheses appearing at any time. This formal language is useful as a proof of concept because the relationship between an open parenthesis and its corresponding close parenthesis is well-defined. This allows us to test our model&rsquo;s ability to predict long-term dependencies. The data for $m = 4, 6, 8$ was provided by John Hewitt. The training datasets and test datasets each contain 10,000 samples, and the validation datasets contain 4,000 samples.</p>
<p>The WikiText-2 dataset, which contains 2 million training tokens and a vocabulary of 33k words, is <a href="https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/">publicly available</a>. We use this data to test our model&rsquo;s performance on natural language modeling tasks. For the natural language model, we load the raw data as a continuous stream; the sequence length is determined by the BPTT parameter, and the batches are constructed so that the hidden state is transferable from one batch to the next. Specifically, each index in a batch is a continuation of the same index in the previous batch.</p>
<h2 id="evaluation-method">Evaluation method</h2>
<p>We use two quantitative evaluation metrics, perplexity and parenthesis prediction accuracy. The formula for parenthesis prediction accuracy is from John Hewitt. The prediction accuracy is measured only for close parentheses, since there is no reason to favor one open parenthesis over another. A prediction is considered correct if at least 80% of the probability mass on <em>any</em> close parenthesis is placed on the true close parenthesis. The distance between an open parenthesis and its corresponding close parenthesis is called closing distance. The long distance prediction accuracy (LDPA) is calculated separately for each possible closing distance. We also report the worst-case prediction accuracy (WCPA), which is the minimum prediction accuracy over all distances.</p>
<h2 id="results">Results</h2>
<p>Table 1 shows that the attention model achieves marginally lower perplexity than the baseline model across $m4$, $m6$, and $m8$. The attention model slightly under-performs relative to the baseline model on WCPA for the $m4$, but performs slightly better on the test set for $m6$ and $m8$.</p>
<p><img src="//localhost:1313/nlp/img/table1.png" alt=""></p>
<p>Overall, the attention model performs similarly to the baseline model on the synthetic data. Although it did not show quantitative improvement over the baseline, our attention model is much more stable during training than the baseline model. The validation WCPA for the baseline model oscillates drastically throughout training but WCPA for the attention model does not. The attention model also trains in fewer epochs than the baseline model.</p>
<p><img src="//localhost:1313/nlp/img/wcpa_path.png" alt=""></p>
<p>Table 2 shows the validation and test results on the WikiText-2 dataset. The attention model with 2 cells and 5 cells both outperform the baseline single-cell model by a significant margin. We suspect the attention model is able to perform better on the WikiText-2 dataset than the parentheses dataset because there are more intricate dependencies between words. The long-distance relationship between parentheses can be captured by just a stack, whereas the relationship between words are harder to capture. It is also possible that the the significant increase in the number of parameters between the baseline and the attention model is responsible for the improved performance.</p>
<p><img src="//localhost:1313/nlp/img/table2.png" alt=""></p>
<h1 id="analysis">Analysis</h1>
<p>In order to capture the differentiation of the LSTM cells in the second-order models, we created heatmaps of sample sentences. The color of the word indicates which LSTM unit was assigned the largest attention score, <code>$s^* = \arg\max_s \alpha_{t,s}$</code>, and the intensity of the color reflects the magnitude of <code>$\alpha_{t, s^*}$</code>. Sample heatmaps for both the formal and natural languages are shown below. The heatmap for the formal language shows that all open parentheses are assigned to one LSTM unit, and all closed parentheses are assigned to the other. This pattern was contrary to our expectations; we expected the two LSTM units to each store the stack for one type of parentheses. However, if the inputs are grammatically correct, a closed parentheses will always indicate a pop from the stack and an open parentheses will always indicate a push to the stack. Therefore, it seems reasonable that one LSTM unit encodes &ldquo;pop&rdquo; and the other encodes &ldquo;push.&rdquo;</p>
<p>The pattern for the natural language heatmap is less discernible. There is no pattern with regard to part of speech or word order within the sentence. The attention is consistent in that individual words are always (softly) assigned to the same cells. Some phrases, like &ldquo;first down&rdquo; and &ldquo;fourth down,&rdquo; are also highlighted consistently. It is possible that a larger number of LSTM units, a larger training set, or fine-tuning of the model parameters would yield a more interpretable pattern. Future work could explore the parameters that yield the best differentiation.</p>
<p><img src="//localhost:1313/nlp/img/heatmaps.png" alt=""></p>
<h1 id="discussion">Discussion</h1>
<p>Overall, the performance of the attention-based second-order LSTM was comparable to the baseline. The attention-based model training on the $m$-bounded Dyck-$k$ dataset learned LDPA and WCPA more quickly and stably than the baseline and trained in fewer epochs. On the WikiText-2 dataset, the attention-based second-order LSTM achieved lower perplexity than baseline (139.5 for 5 hidden cells and 143.6 for 2 hidden cells, compared to 147.2 baseline). However, the parameters are not shared between cells within the attention model, so the attention models contain more parameters than the baseline LSTM.</p>
<p>The primary limitation of this work is that it is limited to LSTM architectures. Ideally, we would have compared a second-order RNN model to a baseline RNN as well as the baseline LSTM. This would allow us to assess whether the second-order model is useful for the parenthesis prediction task; this was difficult to assess with only an LSTM because the baseline LSTM works so well in practice.</p>
<p>Future work in this area could explore how training data size, model size, and parameter tuning (particularly the number of LSTM units) impact model performance.</p>

        </div>

        
        <section data-scroll-index="2">
    <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

    <div class="case-studies">
        <div class="header">
            <h1 class="title">Projects</h1>
            <h2 class="subtitle"></h2>
        </div>
        <div class="case-studies-wrapper">
            
            
            <div class="case-study-item">
                <a href="/argviz/"><img src="https://noa-codes.github.io/img/propensity.png" alt=""></a>
                <h4>Argument Visualization</h4>
                <h5>Causal Inference</h5>
                <p>My team conducted a reanalysis of the data from a published study using causal inference techniques to account for the lack of randomization in the experimental design.</p>
                <a href="/argviz/" class="read-more">Read More</a>
            </div>
            
            <div class="case-study-item">
                <a href="/nlp/"><img src="https://noa-codes.github.io/img/architecture_mini.png" alt=""></a>
                <h4>Second Order LSTMs</h4>
                <h5>Natural Language Processing</h5>
                <p>We test the hypothesis that second-order LSTM architectures have the improved ability (relative to a simple LSTM) to learn long distance dependencies within an input.</p>
                <a href="/nlp/" class="read-more">Read More</a>
            </div>
            
        </div>
    </div>
</section>

        

        
        <footer>
    <div class="icons">
        
        <a href="mailto:nbshtull@stanford.edu">
            <svg width="16px" height="14px" viewBox="0 0 16 14" version="1.1" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink">
                
                <title>ui-16px-1_email-84</title>
                <desc>Created with Sketch.</desc>
                <defs></defs>
                <g id="Portfolio" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                    <g transform="translate(-94.000000, -336.000000)" id="Group" fill="#505050">
                        <g transform="translate(94.000000, 335.000000)">
                            <g id="ui-16px-1_email-84" transform="translate(0.000000, 1.000000)">
                                <path d="M15,0 L1,0 C0.448,0 0,0.447 0,1 L0,13 C0,13.553 0.448,14 1,14 L15,14 C15.552,14 16,13.553 16,13 L16,1 C16,0.447 15.552,0 15,0 Z M14,12 L2,12 L2,5.723 L7.504,8.868 C7.812,9.044 8.189,9.044 8.496,8.868 L14,5.723 L14,12 Z M14,3.42 L8,6.849 L2,3.42 L2,2 L14,2 L14,3.42 Z" id="Shape"></path>
                            </g>
                        </g>
                    </g>
                </g>
            </svg>
        </a>
        
        
        
        
        
        <a href="https://www.linkedin.com/in/noa-bendit-shtull/" target="blank">
            <svg width="17px" height="16px" viewBox="0 0 17 16" version="1.1" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink">
                
                <title>social-16px_logo-linkedin</title>
                <desc>Created with Sketch.</desc>
                <defs></defs>
                <g id="Portfolio" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                    <g transform="translate(-243.000000, -335.000000)" id="Group" fill="#505050">
                        <g transform="translate(94.000000, 335.000000)">
                            <g id="social-16px_logo-linkedin" transform="translate(149.000000, 0.000000)">
                                <path d="M15.3,0 L0.7,0 C0.3,0 0,0.3 0,0.7 L0,15.4 C0,15.7 0.3,16 0.7,16 L15.4,16 C15.8,16 16.1,15.7 16.1,15.3 L16.1,0.7 C16,0.3 15.7,0 15.3,0 Z M4.7,13.6 L2.4,13.6 L2.4,6 L4.8,6 L4.8,13.6 L4.7,13.6 Z M3.6,5 C2.8,5 2.2,4.3 2.2,3.6 C2.2,2.8 2.8,2.2 3.6,2.2 C4.4,2.2 5,2.8 5,3.6 C4.9,4.3 4.3,5 3.6,5 Z M13.6,13.6 L11.2,13.6 L11.2,9.9 C11.2,9 11.2,7.9 10,7.9 C8.8,7.9 8.6,8.9 8.6,9.9 L8.6,13.7 L6.2,13.7 L6.2,6 L8.5,6 L8.5,7 L8.5,7 C8.8,6.4 9.6,5.8 10.7,5.8 C13.1,5.8 13.5,7.4 13.5,9.4 L13.5,13.6 L13.6,13.6 Z" id="Shape"></path>
                            </g>
                        </g>
                    </g>
                </g>
            </svg>
        </a>
        
        
        
        <a href="https://github.com/noa-codes" target="blank">
          <svg width="17px" height="17px" viewBox="0 0 17 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>social-16px_logo-github</title>
              <g id="Portfolio" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                  <g id="GitHub-Mark" transform="translate(-136.000000, -55.000000)" fill="#505050">
                      <path d="M144.319263,55.455 C139.902185,55.455 136.32,59.0366937 136.32,63.4552456 C136.32,66.9897924 138.612029,69.9880428 141.791009,71.0459021 C142.191267,71.1195692 142.337128,70.8725389 142.337128,70.6603777 C142.337128,70.4708077 142.330252,69.967416 142.326323,69.2999922 C140.101086,69.7832483 139.631581,68.2273994 139.631581,68.2273994 C139.267666,67.303123 138.743156,67.057075 138.743156,67.057075 C138.016799,66.5610499 138.798161,66.5708722 138.798161,66.5708722 C139.601132,66.6273503 140.02349,67.3954524 140.02349,67.3954524 C140.737078,68.617835 141.896107,68.2647241 142.351861,68.0599296 C142.424546,67.5432777 142.631305,67.1906579 142.859673,66.9907746 C141.083314,66.7889268 139.215608,66.1023495 139.215608,63.0368165 C139.215608,62.163616 139.527465,61.4490452 140.039206,60.8901576 C139.956698,60.6878187 139.682166,59.8740429 140.117784,58.7729656 C140.117784,58.7729656 140.789137,58.5578577 142.317483,59.5931258 C142.95544,59.4153426 143.640053,59.326942 144.320246,59.3235043 C144.999947,59.326942 145.684069,59.4153426 146.323008,59.5931258 C147.850372,58.5578577 148.520743,58.7729656 148.520743,58.7729656 C148.957343,59.8740429 148.68281,60.6878187 148.600794,60.8901576 C149.113517,61.4490452 149.422919,62.163616 149.422919,63.0368165 C149.422919,66.1102073 147.552266,66.7864712 145.770505,66.9843901 C146.057315,67.2314204 146.313186,67.7195876 146.313186,68.4660808 C146.313186,69.5352357 146.303364,70.3981229 146.303364,70.6603777 C146.303364,70.8745034 146.447751,71.1234981 146.853411,71.045411 C150.029936,69.9850961 152.32,66.9888101 152.32,63.4552456 C152.32,59.0366937 148.737815,55.455 144.319263,55.455" id="Fill-4"></path>
                  </g>
              </g>
          </svg>
        </a>
        
    </div>

    <a href="#" style="color: #574cdd; text-decoration: underline;" data-scroll-goto="0">Back to top</a>
</footer>
        

        
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-XXXXXXXX-Y', 'auto');
	
	ga('send', 'pageview');
}
</script>

<script src="https://noa-codes.github.io/js/jquery-1.js"></script>
<script src="https://noa-codes.github.io/js/stickUp.js"></script>
<script src="https://noa-codes.github.io/js/scrollIt.js" type="text/javascript"></script>
<script type="text/javascript">
    jQuery(function($) {
        $(document).ready( function() {
            $('.main-navigation').stickUp({

            });
        });
    });
    $(function(){
        $.scrollIt({
            topOffset: -70,         
            easing: 'ease-in-out'
        });
    });
</script>
    </body>
</html>