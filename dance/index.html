<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Video Action Recognition</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="theme-color" content="#D50000">
<meta name="description" content="">
<meta property="og:description" content="">
<meta name="twitter:description" content="">
<meta property="og:image" content="pose.png">
<meta name="twitter:image" content="pose.png">
<link rel="icon" href="https://noa-codes.github.io/img/favicon.png" type="image/x-icon">
<link rel="shortcut icon" href="https://noa-codes.github.io/img/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="https://noa-codes.github.io/css/final.css">
<link rel="stylesheet" href="https://noa-codes.github.io/css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Merriweather" rel="stylesheet">

    </head>
    <body>
        <nav class="main-navigation stuckMenu">
    <a class="logo" href="https://noa-codes.github.io/">Noa Bendit-Shtull</a>
    
    <div class="sub-navigation">
        <a href="https://noa-codes.github.io/" class="main-navigation-link" data-scroll-goto="2">Projects</a>
    </div>
</nav>
        <section>
    <div class="main-case-study">
        <h1 class="title" style="text-align: center;">Sequential Neural Networks for Classifying Dance Styles from Video</h1>
        <h2 class="subtitle" style="text-align: center;">Video Action Recognition</h2>
        <img src="img/pipeline.png" alt="">
        <p class="summary">Many video action recognition models rely on background information or context clues; these models are natural extensions of image processing, where only spatial information is needed. Our project focuses on classification of dance styles from video, a task that requires representation of motion because contextual information such as scenery or props is often sparse. We use Castro et al.&#39;s Let&#39;s Dance dataset, designed specifically for this purpose. Our contribution is the application of sequential models to this dataset. We show that, while sequential models out-perform purely spatial models by roughly 10% accuracy, they are not as successful in dance style recognition as Castro&#39;s models, which use optical flow to explicitly encode motion.</p>
        <div class="case-details">
            <div class="case-details-item">
                <p><strong>Team</strong>
                    
                        
                        <br><a href="https://www.linkedin.com/in/tiffany-cheng14">Tiffany Cheng</a>
                        
                    
                        
                        <br><a href="https://www.linkedin.com/in/julie-wang-66387421/">Julie Wang</a>
                        
                    
                        
                        <br>Noa Bendit-Shtull
                        
                    
                </p>
            </div>

        </div>
        <h1 style="color: black; font-family: merriweather; font-size: 26px; line-height: 1.7em; margin-top: 20px;"></h1>
        <div class="case-links">
            
            
            <div class="link-div">
                <a href="/dance.pdf" target="blank" class="case-detail-link">
                    
                    <img style="top:1.8px;" src="https://noa-codes.github.io/svg/download.svg">
                    
                    Download Final Deliverable
                </a>
            </div>
            
        </div>
    </div>
</section>

        <div class="content-case-study">
            <h1 id="study-overview">Study Overview</h1>
<p>Identifying highly dynamic actions from video is a complicated task in visual recognition, with applications ranging from automatic video tagging to collision avoidance systems. Humans display almost full range of motion through dance, so identifying dance styles is an excellent case study for testing human motion identification methods.</p>
<p>Our project is an action recognition task which seeks to classify video clips as one of ten types of dance: ballet, break dancing, flamenco, foxtrot, latin, quickstep, square, swing, tango, or waltz. For this task, we use the <a href="https://www.cc.gatech.edu/cpl/projects/dance/">Let&rsquo;s Dance dataset</a> presented by <a href="https://arxiv.org/abs/1801.07388">Castro et al.</a>, which is comprised of 10-second video clips from YouTube. Some of these video clips occur in very similar settings, which makes them challenging to differentiate based on background imagery alone. For example, the dances below (tango, waltz, and foxtrot) are very difficult to distinguish.</p>
<p><img src="/dance/img/tango-waltz-foxtrot.png" alt=""></p>
<p>As a result, a successful model will need to encode movement or pose information. We assess the effectiveness of several sequential models, including a traditional LSTM, in modeling dynamic movement. We find that, although these models achieve significantly better performance than the baseline frame-by-frame approach, they don&rsquo;t achieve the accuracy of Castro&rsquo;s temporal models.</p>
<h1 id="approach">Approach</h1>
<h2 id="baseline">Baseline</h2>
<p>The traditional baseline method for action classification from video is a spatial model&mdash;that is, a frame-by-frame model with no temporal component. A CNN is trained on individual frames of a video. At test time, all frames of a video are individually classified, and the video&rsquo;s final classification is the majority class of its frames. We compare our results to the frame by frame model introduced by <a href="https://arxiv.org/abs/1801.07388">Castro et al.</a>, which uses a pre-trained variant of CaffeNet that has been finetuned on the Let&rsquo;s Dance dataset. This model achieves test accuracy of 56.4%.</p>
<h2 id="sequential-models">Sequential Models</h2>
<p>Like Castro et al., our method utilizes a two-stream late fusion model, shown in the figure at the top of the page. The two streams of data are RGB images and PoseNet skeleton data. Features are extracted from each RGB frame using Pytorch&rsquo;s pretrained ResNet-18 model with the final layer converted to an identity layer, resulting in 512-dimensional features. Features are extracted from the PoseNet data using a shallow CNN with small filters adapted from <a href="https://arxiv.org/abs/1909.03466">Khalid and Yu (2019)</a>. The two streams of features are then concatenated.</p>
<p>To create sequences from these features, we subsample the original 30fps video clips, with the subsampling rate determined through hyperparameter tuning. This creates a sequence of length anywhere between 2 and 300 for each 10-second clip. This sequence of combined data is then input to one of three sequential models: an LSTM, an LSTM with self-attention, and a temporal convolutional neural network (TCN) from <a href="https://arxiv.org/abs/1803.01271">Bai et al.</a>.</p>
<p>In our LSTM, the addition of self-attention allows the model to determine which frames in a sequence should receive the most weight in the output. The attention mechanism used is scaled dot product self-attention, adapted from <a href="https://arxiv.org/abs/1706.03762">Vaswani et al. (2017)</a>. The authors advocate for using self-attention as opposed to the recurrent and convolutional layers commonly used for attention because it is computationally less complex and faster to compute. Self-attention also yields more interpretable models. The architecture is shown below.</p>
<p><img src="/dance/img/attention-lstm2.png" alt=""></p>
<p>We chose the TCN because, unlike RNNs, it doesn&rsquo;t suffer from the vanishing gradient problem; the gradient flows back through the convolutional layers of the TCN, rather than back through the sequence. The TCN also has many fewer parameters than even a single layer LSTM. The structure of the TCN is shown below.</p>
<p><img src="/dance/img/diagrams-TCN.png" alt="">
<em>Architecture of a TCN. The left-hand side shows a three-layer TCN with kernel size 3 and exponential dilation in each layer. The right-hand side shows the detail of each layer, including two dilated causal convolutions and a residual connection. Based on Figure 1 from <a href="https://arxiv.org/abs/1803.01271">Bai et al.</a>.</em></p>
<h1 id="experiments">Experiments</h1>
<h2 id="data">Data</h2>
<p>The Let&rsquo;s Dance video dataset by Castro et al. contains roughly 1,000 videos spanning 10 dynamic and visually overlapping dance types. There are approximately 100 videos per dance style, extracted from YouTube in 10-second clips at 30 frames per second. Each video is available in its original RGB format and as extracted pose coordinates.</p>
<p>The data was randomized at the video level into 80% training, 10% validation, and 10% test data. The same splits were used for the RGB data and the pose data.</p>
<h2 id="results">Results</h2>
<p>Our quantitative results, relative to the frame-by-frame baseline from Castro et al., are shown in the table below. Although the frame-by-frame model is the most relevant, it is not a perfect comparison. Castro&rsquo;s frame-by-frame model makes predictions for every frame in a given video clip, and then uses plurality voting to classify the video as a whole. In contrast, our sequential models use frame sub-sampling to reduce the length of the input sequence, so less information is used for training. Furthermore, all of our models utilize the extracted pose information, which Castro&rsquo;s frame-by-frame baseline does not.</p>
<p><img src="/dance/img/results.png" alt=""></p>
<h1 id="analysis">Analysis</h1>
<h2 id="lstm">LSTM</h2>
<p>The LSTM model achieves testing accuracy of 65.9%, ten percentage points higher than the baseline frame-by-frame model. However, the accuracy is still several percentage points behind Castro&rsquo;s multi-stream CNNs that incorporate optical flow.</p>
<p>The dropout rate used to train this model is 0.70; because the data set is so small (less than 1,000 video clips in total), even a small model can be expressive enough to over-fit the training data. Therefore, aggressive dropout is needed to ensure that the model generalizes to the test data.</p>
<p>Surprisingly, the hyperparameter tuning chose sub-sampling of every 60 frames, meaning that each 10-second video clip was reduced to a sequence of 5 frames, one every two seconds. This level of sub-sampling is surprising because it can&rsquo;t encode dance movements faster than two seconds. On the other hand, it&rsquo;s likely that processing very short sequences greatly reduced or eliminated the vanishing gradient problem.</p>
<p><img src="/dance/img/confusion_matrices.png" alt=""></p>
<p>The figure above shows the confusion matrix for each model. The dance styles predicted with the greatest accuracy are ballet, break dancing, and square dancing. Ballet and break dancing, unlike the other dance forms, are not partner dances, so they are visually distinctive. Although square dancing is a partner dance, large portions of the dance are performed in groups, for example by joining hands in a circle. This may explain why square dancing is always correctly identified, and is never confused with other dance forms. The highlighted area in the bottom right corner of the confusion matrix indicates that tango and waltz are often confused; of the ten tango videos, half were mistakenly classified as waltz.</p>
<h2 id="lstm-with-self-attention">LSTM with Self-Attention</h2>
<p>We expected self-attention to enhance the performance of the LSTM by directing weight to the most important frames. However, the LSTM with attention achieves accuracy of 63.7%, slightly lower than the standard LSTM. Given that our test data set only consists of 91 video clips (10% of the original ~1000 videos), this performance is comparable. The confusion matrix is similar to the confusion matrix for the LSTM. Relative to the plain LSTM, however, it is more successful at differentiating tango and waltz; perhaps the attention mechanism was useful in differentiating visually similar ballroom dances. There were no dance types where the TCN outperformed both LSTM models in identifying the right videos.</p>
<p>Even though classification accuracy was comparable between the LSTM model and the LSTM with attention model, hyperparameter tuning yielded pretty different parameters. While hyperparameter tuning chose a very aggressive dropout rate for the LSTM, it selected a dropout rate of 0.05 for the self-attention model. Also, it preferred a smaller frame subsampling rate of every 30 frames for the self-attention model, meaning that each 10-second video clip was reduced to a sequence of 10 frames (one per second). This yields a sequence with double the number of frames as compared to the LSTM. Perhaps a smaller sampling rate was preferred because the attention model is able to handle longer sequences of frames since it can upweight more important frames and downweigh less important ones.</p>
<h2 id="temporal-convolutional-network-tcn">Temporal Convolutional Network (TCN)</h2>
<p>The TCN model achieves accuracy of 58.2% on the test data, only marginally better than the frame-by-frame model. Although the TCN performed well on the validation set, achieving 75% accuracy, it did not generalize well to the test data. The confusion matrix shows that, like the LSTM models, the TCN performs well on ballet, break dancing, and square dancing, likely due to the distinctive features of those dances. However, it struggled more than the LSTM models in identifying the tango videos, instead mislabelling most of them as waltz.</p>
<p>One hypothesis for the TCN&rsquo;s subpar performance is that it is not expressive enough to capture differences in movement between the dances; the LSTM models involve far more parameters per layer than the TCN. Perhaps if we had increased the complexity of the TCN by adding hidden layers, reducing the regularization, etc. its performance would have improved.</p>
<h1 id="discussion">Discussion</h1>
<p>Surprisingly, the plain LSTM model out-performed both the LSTM with attention and the TCN model. We had expected that attention would help direct the focus to key frames, and that the TCN would alleviate potential problems with vanishing gradients. However, given that the LSTM performed best with only 5 frames from a 10-second video clip, vanishing gradients were not problematic.</p>
<p>For this project, we chose not to use optical flow to encode the temporal dimension in order to determine how much a sequential model could capture on its own. However, we realized that optical flow is an important driver in the success of action recognition on highly dynamic motion. Given more time, we would set up a pipeline to process the videos&rsquo; optical flow data and expand our models to be three-stream.</p>
<p>Ultimately, one of the biggest constraints was the limited data set. Since there were only around 1,000 videos in total, the validation set and test set each only contained 100 observations. Castro \etal circumvented this problem by processing videos in 16-frame chunks, but we opted to use each video clip as a single input to our sequential models. Therefore, an important direction for future work is to use data augmentation to increase the data set size. Future work could also accommodate this smaller dataset by exploring alternative regularization strategies,such as normalization layers for the LSTM models.</p>

        </div>

        
        <section data-scroll-index="2">
    <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

    <div class="case-studies">
        <div class="header">
            <h1 class="title">Projects</h1>
            <h2 class="subtitle"></h2>
        </div>
        <div class="case-studies-wrapper">
            
            
            <div class="case-study-item">
                <a href="https://noa-codes.github.io/argviz/"><img src="https://noa-codes.github.io/img/propensity.png" alt=""></a>
                <h4>Argument Visualization</h4>
                <h5>Causal Inference</h5>
                <p>My team conducted a reanalysis of the data from a published study using causal inference techniques to account for the lack of randomization in the experimental design.</p>
                <a href="https://noa-codes.github.io/argviz/" class="read-more">Read More</a>
            </div>
            
            <div class="case-study-item">
                <a href="https://noa-codes.github.io/nlp/"><img src="https://noa-codes.github.io/img/architecture_mini.png" alt=""></a>
                <h4>Second Order LSTMs</h4>
                <h5>Natural Language Processing</h5>
                <p>We test the hypothesis that second-order LSTM architectures have the improved ability (relative to a simple LSTM) to learn long distance dependencies within an input.</p>
                <a href="https://noa-codes.github.io/nlp/" class="read-more">Read More</a>
            </div>
            
            <div class="case-study-item">
                <a href="https://noa-codes.github.io/dance/"><img src="https://noa-codes.github.io/img/pose.png" alt=""></a>
                <h4>Dance Classification</h4>
                <h5>Video Action Recognition</h5>
                <p>We assess the ability of sequential neural networks to capture dynamic movement and predict dance styles from video.</p>
                <a href="https://noa-codes.github.io/dance/" class="read-more">Read More</a>
            </div>
            
        </div>
    </div>
</section>

        

        
        <footer>
    <div class="icons">
        
        <a href="mailto:nbshtull@stanford.edu">
            <svg width="16px" height="14px" viewBox="0 0 16 14" version="1.1" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink">
                
                <title>ui-16px-1_email-84</title>
                <desc>Created with Sketch.</desc>
                <defs></defs>
                <g id="Portfolio" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                    <g transform="translate(-94.000000, -336.000000)" id="Group" fill="#505050">
                        <g transform="translate(94.000000, 335.000000)">
                            <g id="ui-16px-1_email-84" transform="translate(0.000000, 1.000000)">
                                <path d="M15,0 L1,0 C0.448,0 0,0.447 0,1 L0,13 C0,13.553 0.448,14 1,14 L15,14 C15.552,14 16,13.553 16,13 L16,1 C16,0.447 15.552,0 15,0 Z M14,12 L2,12 L2,5.723 L7.504,8.868 C7.812,9.044 8.189,9.044 8.496,8.868 L14,5.723 L14,12 Z M14,3.42 L8,6.849 L2,3.42 L2,2 L14,2 L14,3.42 Z" id="Shape"></path>
                            </g>
                        </g>
                    </g>
                </g>
            </svg>
        </a>
        
        
        
        
        
        <a href="https://www.linkedin.com/in/noa-bendit-shtull/" target="blank">
            <svg width="17px" height="16px" viewBox="0 0 17 16" version="1.1" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink">
                
                <title>social-16px_logo-linkedin</title>
                <desc>Created with Sketch.</desc>
                <defs></defs>
                <g id="Portfolio" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                    <g transform="translate(-243.000000, -335.000000)" id="Group" fill="#505050">
                        <g transform="translate(94.000000, 335.000000)">
                            <g id="social-16px_logo-linkedin" transform="translate(149.000000, 0.000000)">
                                <path d="M15.3,0 L0.7,0 C0.3,0 0,0.3 0,0.7 L0,15.4 C0,15.7 0.3,16 0.7,16 L15.4,16 C15.8,16 16.1,15.7 16.1,15.3 L16.1,0.7 C16,0.3 15.7,0 15.3,0 Z M4.7,13.6 L2.4,13.6 L2.4,6 L4.8,6 L4.8,13.6 L4.7,13.6 Z M3.6,5 C2.8,5 2.2,4.3 2.2,3.6 C2.2,2.8 2.8,2.2 3.6,2.2 C4.4,2.2 5,2.8 5,3.6 C4.9,4.3 4.3,5 3.6,5 Z M13.6,13.6 L11.2,13.6 L11.2,9.9 C11.2,9 11.2,7.9 10,7.9 C8.8,7.9 8.6,8.9 8.6,9.9 L8.6,13.7 L6.2,13.7 L6.2,6 L8.5,6 L8.5,7 L8.5,7 C8.8,6.4 9.6,5.8 10.7,5.8 C13.1,5.8 13.5,7.4 13.5,9.4 L13.5,13.6 L13.6,13.6 Z" id="Shape"></path>
                            </g>
                        </g>
                    </g>
                </g>
            </svg>
        </a>
        
        
        
        <a href="https://github.com/noa-codes" target="blank">
          <svg width="17px" height="17px" viewBox="0 0 17 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>social-16px_logo-github</title>
              <g id="Portfolio" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                  <g id="GitHub-Mark" transform="translate(-136.000000, -55.000000)" fill="#505050">
                      <path d="M144.319263,55.455 C139.902185,55.455 136.32,59.0366937 136.32,63.4552456 C136.32,66.9897924 138.612029,69.9880428 141.791009,71.0459021 C142.191267,71.1195692 142.337128,70.8725389 142.337128,70.6603777 C142.337128,70.4708077 142.330252,69.967416 142.326323,69.2999922 C140.101086,69.7832483 139.631581,68.2273994 139.631581,68.2273994 C139.267666,67.303123 138.743156,67.057075 138.743156,67.057075 C138.016799,66.5610499 138.798161,66.5708722 138.798161,66.5708722 C139.601132,66.6273503 140.02349,67.3954524 140.02349,67.3954524 C140.737078,68.617835 141.896107,68.2647241 142.351861,68.0599296 C142.424546,67.5432777 142.631305,67.1906579 142.859673,66.9907746 C141.083314,66.7889268 139.215608,66.1023495 139.215608,63.0368165 C139.215608,62.163616 139.527465,61.4490452 140.039206,60.8901576 C139.956698,60.6878187 139.682166,59.8740429 140.117784,58.7729656 C140.117784,58.7729656 140.789137,58.5578577 142.317483,59.5931258 C142.95544,59.4153426 143.640053,59.326942 144.320246,59.3235043 C144.999947,59.326942 145.684069,59.4153426 146.323008,59.5931258 C147.850372,58.5578577 148.520743,58.7729656 148.520743,58.7729656 C148.957343,59.8740429 148.68281,60.6878187 148.600794,60.8901576 C149.113517,61.4490452 149.422919,62.163616 149.422919,63.0368165 C149.422919,66.1102073 147.552266,66.7864712 145.770505,66.9843901 C146.057315,67.2314204 146.313186,67.7195876 146.313186,68.4660808 C146.313186,69.5352357 146.303364,70.3981229 146.303364,70.6603777 C146.303364,70.8745034 146.447751,71.1234981 146.853411,71.045411 C150.029936,69.9850961 152.32,66.9888101 152.32,63.4552456 C152.32,59.0366937 148.737815,55.455 144.319263,55.455" id="Fill-4"></path>
                  </g>
              </g>
          </svg>
        </a>
        
    </div>

    <a href="#" style="color: #574cdd; text-decoration: underline;" data-scroll-goto="0">Back to top</a>
</footer>
        

        
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-XXXXXXXX-Y', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script src="https://noa-codes.github.io/js/jquery-1.js"></script>
<script src="https://noa-codes.github.io/js/stickUp.js"></script>
<script src="https://noa-codes.github.io/js/scrollIt.js" type="text/javascript"></script>
<script type="text/javascript">
    jQuery(function($) {
        $(document).ready( function() {
            $('.main-navigation').stickUp({

            });
        });
    });
    $(function(){
        $.scrollIt({
            topOffset: -70,         
            easing: 'ease-in-out'
        });
    });
</script>
    </body>
</html>