<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Noa Bendit-Shtull</title>
    <link>https://noa-codes.github.io/</link>
    <description>Recent content on Noa Bendit-Shtull</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://noa-codes.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Causal Inference</title>
      <link>https://noa-codes.github.io/argviz/</link>
      <pubDate>Sun, 15 Oct 2017 03:29:08 -0700</pubDate>
      
      <guid>https://noa-codes.github.io/argviz/</guid>
      <description>Study Overview The paper Improving analytical reasoning and argument understanding: a quasi-experimental field study of argument visualization (Cullen et al. 2018) studies the impact of training in argument visualization on analytical reasoning. The authors hypothesized that training students to visualize the logical structure of presented arguments would lead to improvements on the logical reasoning section of the LSAT. To investigate this hypothesis, they created a 12 week seminar course for freshman students at Princeton consisting of group and individual practice of visualizing arguments.</description>
    </item>
    
    <item>
      <title>Natural Language Processing</title>
      <link>https://noa-codes.github.io/nlp/</link>
      <pubDate>Sun, 15 Oct 2017 03:29:08 -0700</pubDate>
      
      <guid>https://noa-codes.github.io/nlp/</guid>
      <description>Study Overview In the basic RNN, the vanishing gradient problem limits a language model&amp;rsquo;s memory; a range of models, such as LSTM and GRU, have been developed to reduce this friction. In order to preserve long-term memory, these models must determine which information to store from previous hidden states. This introduces a related problemâ€“if the model chooses the wrong information to carry in its long-term memory, it may be difficult to recover.</description>
    </item>
    
    <item>
      <title>Video Action Recognition</title>
      <link>https://noa-codes.github.io/dance/</link>
      <pubDate>Sun, 15 Oct 2017 03:29:08 -0700</pubDate>
      
      <guid>https://noa-codes.github.io/dance/</guid>
      <description>Study Overview Identifying highly dynamic actions from video is a complicated task in visual recognition, with applications ranging from automatic video tagging to collision avoidance systems. Humans display almost full range of motion through dance, so identifying dance styles is an excellent case study for testing human motion identification methods.
Our project is an action recognition task which seeks to classify video clips as one of ten types of dance: ballet, break dancing, flamenco, foxtrot, latin, quickstep, square, swing, tango, or waltz.</description>
    </item>
    
  </channel>
</rss>
